---
title: Scraping PDFs with R
author: Keith Hultman
date: '2017-08-15'
slug: scraping-pdfs-with-r
categories:
  - articles
tags:
  - data
  - R
  - data4democracy
---

I wanted to explore data related to confederate monuments, and the Southern Poverty Law Center had a comprehensive report on the location of more than 700 confederate monuments on public property. However, this data was locked away in a PDF file, and I could not find any easily accessible data sources of similar scope. 

Try pdftools package

```{r}
library(pdftools)

confederate_txt <- pdf_text(pdf = "~/datasets/whoseheritage_splc.pdf")
```

The confederate_txt object is a character vector of length 44, which should contain a character string for each page in the document. The original file had 44 pages with the first two pages using roman numerals for page numbers. 

The tables of data are on pages 17-35 of the original document, which mean they should be on 19-37 of our character vector. Let's check the first page of tables to see if this is where it starts.

```{r}
page <- confederate_txt[19] 
print(page)

cat(page)

pages <- confederate_txt[19:35]

pages2 <- toString(pages)

write_file(pages2, "test.txt")
```

Looking at the output of this string shows that the columns are ignored by the pdf_text extractor tool. It should be easy to parse out the state names using the double slash, but we will have to extract these data by columns. 

```{r}
#library(stringr)
library(tidyverse)

page2 <- pages2 %>% gsub(pattern = " {3,}", replacement = "\t") %>% 
  gsub(pattern = "(\\d{4}|N/A)", replacement = "\\1\r") %>% 
  gsub(pattern = "LIST OF PUBLICLY SUPPORTED SPACES DEDICATED TO THE CONFEDERACY", replacement = "") %>% 
  gsub(pattern = "special report \\| whose heritage\\?\t\\d+\n", replacement = "") %>% 
  gsub(pattern = "\\d+\tsouthern poverty law center\n", replacement = "") %>% 
  gsub(pattern = "(\\w+) // .*?\t", replacement = "", perl = TRUE)


page3 <- page2 %>% gsub(pattern = "(\r)\n", replacement = "\\1") %>% 
  gsub(pattern = "(\r)\t", replacement = "\\1") %>% 
  gsub(pattern = "(\r)\t", replacement = "\\1") %>% 
  gsub(pattern = "(\r) ", replacement = "\\1")

page3

page_spl <- strsplit(page3, split = "\r")[[1]] %>% 
  gsub(pattern = ".*\n", replacement = "") %>% 
  gsub(pattern = "^\t", replacement = "") %>% 
  str_trim()

page_spl

maptest <- map(memorial_types, ~str_detect(page_spl,.))

memorial_types <- c("Monument", 
                    "Park",
                    "School",
                    "Road/Street/Highway",
                    "County",
                    "Municipality",
                    "Military Base",
                    "River/Creek/Lake",
                    "Flag",
                    "Holiday",
                    "Commemorative",
                    "Campground")

page_df <- as.data.frame(page_spl) %>% 
  mutate(year = str_extract(page_spl, pattern = "\\d{1,4}$")) %>% 
  mutate(city = str_extract(page_spl, pattern = "^.+?\t"))

library(magrittr)
type_df = as.data.frame(map(memorial_types, ~str_detect(page_spl,.))) %>% 
  set_colnames(memorial_types) %>% 
  gather(key = "Type", value = "Value") %>% 
  filter(Value == TRUE)





type_df
page_df

year <- str_extract(page_spl, pattern = "\\d{1,4}$")

city <- str_extract(page_spl, )

grep("\\r(\\*+\\t\\*+\\t\\*+\\t)(\\d{4}|N/A)", page4, perl = TRUE)


shopping_list <- c("apples x4", "bag of flour", "bag of sugar", "milk x2")
shopping_list
str_extract(shopping_list, "\\d")

page_tbl <- read_table(page_spl)

page_spl2 <- unlist(map(page_spl, ~strsplit(., split = " {2}", perl = TRUE)))

page_spl2[[92]]
```

